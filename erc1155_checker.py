#!/usr/bin/env python3
"""
High-performance multithreaded proxy checker.

Features:
- Thread-local sessions with connection pooling
- Robust proxy normalization
- Exponential backoff with jitter
- Graceful shutdown on Ctrl+C
- Clean logging and progress reporting
"""

from __future__ import annotations

import argparse
import logging
import random
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Optional

import requests
import urllib3
from requests.adapters import HTTPAdapter
from tqdm import tqdm

# ------------------- Global Settings -------------------
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}

# ------------------- Logging -------------------
class ColorFormatter(logging.Formatter):
    COLORS = {
        logging.DEBUG: "\033[37m",
        logging.INFO: "\033[36m",
        logging.WARNING: "\033[33m",
        logging.ERROR: "\033[31m",
        logging.CRITICAL: "\033[41m",
    }
    RESET = "\033[0m"

    def format(self, record: logging.LogRecord) -> str:
        color = self.COLORS.get(record.levelno, self.RESET)
        return f"{color}{super().format(record)}{self.RESET}"


def setup_logger(verbose: bool) -> logging.Logger:
    logger = logging.getLogger("proxy_checker")
    logger.setLevel(logging.DEBUG if verbose else logging.INFO)

    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(
            ColorFormatter("%(asctime)s | %(levelname)-8s | %(message)s")
        )
        logger.addHandler(handler)

    logger.propagate = False
    return logger


# ------------------- Thread-local Session -------------------
_thread_local = threading.local()


def get_session() -> requests.Session:
    if not hasattr(_thread_local, "session"):
        session = requests.Session()
        session.verify = False
        session.headers.update(DEFAULT_HEADERS)

        adapter = HTTPAdapter(
            pool_connections=50,
            pool_maxsize=50,
            max_retries=0,
        )
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        _thread_local.session = session

    return _thread_local.session


# ------------------- Proxy Utilities -------------------
def normalize_proxy(proxy: str) -> Optional[Dict[str, str]]:
    proxy = proxy.strip()
    if not proxy:
        return None

    if "://" not in proxy:
        proxy = f"http://{proxy}"

    return {
        "http": proxy,
        "https": proxy,
    }


def read_proxies(path: str, logger: logging.Logger) -> List[str]:
    file = Path(path)
    if not file.is_file():
        logger.error("Proxy file not found: %s", path)
        return []

    proxies: set[str] = set()
    with file.open(encoding="utf-8", errors="ignore") as f:
        for line in f:
            p = line.strip()
            if normalize_proxy(p):
                proxies.add(p)

    logger.info("Loaded %d unique proxies", len(proxies))
    return sorted(proxies)


def write_proxies(path: str, proxies: List[str], logger: logging.Logger) -> None:
    out = Path(path)
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text("\n".join(proxies), encoding="utf-8")
    logger.info("Saved %d proxies → %s", len(proxies), path)


# ------------------- Proxy Checking -------------------
def check_proxy(
    proxy: str,
    test_url: str,
    retries: int,
    timeout: int,
    base_delay: float,
    logger: logging.Logger,
) -> Optional[str]:
    proxy_cfg = normalize_proxy(proxy)
    if not proxy_cfg:
        return None

    session = get_session()

    for attempt in range(1, retries + 1):
        try:
            start = time.perf_counter()
            r = session.get(
                test_url,
                proxies=proxy_cfg,
                timeout=timeout,
            )
            elapsed = time.perf_counter() - start

            if r.status_code == 200:
                logger.debug("OK %s | %.2fs", proxy, elapsed)
                return proxy

            logger.debug(
                "[%d/%d] HTTP %d | %s",
                attempt,
                retries,
                r.status_code,
                proxy,
            )

        except Exception as e:
            logger.debug("[%d/%d] FAIL %s | %s", attempt, retries, proxy, e)

        delay = base_delay * (2 ** (attempt - 1))
        delay *= 1 + random.random() * 0.4
        time.sleep(delay)

    return None


def validate_proxies(
    proxies: List[str],
    test_url: str,
    max_workers: int,
    retries: int,
    timeout: int,
    delay: float,
    logger: logging.Logger,
) -> List[str]:
    if not proxies:
        return []

    total = len(proxies)
    max_workers = min(max_workers, 150 if total > 200 else max_workers)

    logger.info("Checking %d proxies using %d threads", total, max_workers)

    valid: List[str] = []
    stop_event = threading.Event()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(
                check_proxy,
                proxy,
                test_url,
                retries,
                timeout,
                delay,
                logger,
            ): proxy
            for proxy in proxies
        }

        try:
            with tqdm(total=total, unit="proxy", ncols=100) as bar:
                for future in as_completed(futures):
                    if stop_event.is_set():
                        break

                    result = future.result()
                    if result:
                        valid.append(result)

                    bar.set_postfix(valid=len(valid))
                    bar.update(1)

        except KeyboardInterrupt:
            stop_event.set()
            logger.warning("Interrupted by user, stopping checks…")

    logger.info("Done: %d / %d valid", len(valid), total)
    return valid


# ------------------- Main -------------------
def main() -> None:
    parser = argparse.ArgumentParser(description="Fast multithreaded proxy checker")
    parser.add_argument("input_file")
    parser.add_argument("output_file")
    parser.add_argument("--test_url", default="https://httpbin.org/ip")
    parser.add_argument("--max_workers", type=int, default=50)
    parser.add_argument("--retries", type=int, default=3)
    parser.add_argument("--timeout", type=int, default=5)
    parser.add_argument("--delay", type=float, default=0.25)
    parser.add_argument("--verbose", action="store_true")

    args = parser.parse_args()
    logger = setup_logger(args.verbose)

    start = time.perf_counter()

    proxies = read_proxies(args.input_file, logger)
    if not proxies:
        logger.warning("No proxies to check.")
        return

    valid = validate_proxies(
        proxies,
        args.test_url,
        args.max_workers,
        args.retries,
        args.timeout,
        args.delay,
        logger,
    )

    write_proxies(args.output_file, valid, logger)
    logger.info("Finished in %.2fs", time.perf_counter() - start)


if __name__ == "__main__":
    main()
